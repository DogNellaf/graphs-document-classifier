{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a comparison of the original code and after revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requried modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in .\\.venv\\lib\\site-packages (3.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in .\\.venv\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in .\\.venv\\lib\\site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in .\\.venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in .\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in .\\.venv\\lib\\site-packages (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in .\\.venv\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in .\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in .\\.venv\\lib\\site-packages (from ipywidgets) (8.32.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in .\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in .\\.venv\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in .\\.venv\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in .\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in .\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in .\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in .\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in .\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in .\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in .\\.venv\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in .\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in .\\.venv\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in .\\.venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in .\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in .\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Jinja2 in .\\.venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\lib\\site-packages (from Jinja2) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyyaml in .\\.venv\\lib\\site-packages (6.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install networkx\n",
    "%pip install matplotlib\n",
    "%pip install tqdm\n",
    "%pip install numpy\n",
    "%pip install ipywidgets\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install Jinja2\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in .\\.venv\\lib\\site-packages (2.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in .\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in .\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in .\\.venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in .\\.venv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in .\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray in .\\.venv\\lib\\site-packages (2.42.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click>=7.0 in .\\.venv\\lib\\site-packages (from ray) (8.1.8)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from ray) (3.17.0)\n",
      "Requirement already satisfied: jsonschema in .\\.venv\\lib\\site-packages (from ray) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in .\\.venv\\lib\\site-packages (from ray) (1.1.0)\n",
      "Requirement already satisfied: packaging in .\\.venv\\lib\\site-packages (from ray) (24.2)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in .\\.venv\\lib\\site-packages (from ray) (5.29.3)\n",
      "Requirement already satisfied: pyyaml in .\\.venv\\lib\\site-packages (from ray) (6.0.2)\n",
      "Requirement already satisfied: aiosignal in .\\.venv\\lib\\site-packages (from ray) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in .\\.venv\\lib\\site-packages (from ray) (1.5.0)\n",
      "Requirement already satisfied: requests in .\\.venv\\lib\\site-packages (from ray) (2.32.3)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from click>=7.0->ray) (0.4.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in .\\.venv\\lib\\site-packages (from jsonschema->ray) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in .\\.venv\\lib\\site-packages (from jsonschema->ray) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in .\\.venv\\lib\\site-packages (from jsonschema->ray) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in .\\.venv\\lib\\site-packages (from jsonschema->ray) (0.23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests->ray) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests->ray) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests->ray) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests->ray) (2025.1.31)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in .\\.venv\\lib\\site-packages (from referencing>=0.28.4->jsonschema->ray) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[distributed] in .\\.venv\\lib\\site-packages (2025.2.0)\n",
      "Requirement already satisfied: click>=8.1 in .\\.venv\\lib\\site-packages (from dask[distributed]) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in .\\.venv\\lib\\site-packages (from dask[distributed]) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in .\\.venv\\lib\\site-packages (from dask[distributed]) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\lib\\site-packages (from dask[distributed]) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in .\\.venv\\lib\\site-packages (from dask[distributed]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in .\\.venv\\lib\\site-packages (from dask[distributed]) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in .\\.venv\\lib\\site-packages (from dask[distributed]) (1.0.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in .\\.venv\\lib\\site-packages (from dask[distributed]) (8.6.1)\n",
      "Requirement already satisfied: distributed==2025.2.0 in .\\.venv\\lib\\site-packages (from dask[distributed]) (2025.2.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (3.1.5)\n",
      "Requirement already satisfied: locket>=1.0.0 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (7.0.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (6.4.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (2.3.0)\n",
      "Requirement already satisfied: zict>=3.0.0 in .\\.venv\\lib\\site-packages (from distributed==2025.2.0->dask[distributed]) (3.0.0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from click>=8.1->dask[distributed]) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in .\\.venv\\lib\\site-packages (from importlib_metadata>=4.13.0->dask[distributed]) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\lib\\site-packages (from jinja2>=2.10.3->distributed==2025.2.0->dask[distributed]) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"dask[distributed]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import yaml\n",
    "import json\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from subprocess import CalledProcessError, TimeoutExpired\n",
    "from multiprocessing import Lock\n",
    "from os import makedirs, replace, cpu_count, getpid\n",
    "from os.path import join, exists\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.original.main import do_operations as ORIGINAL_SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.paralized.main import do_operations as PARALLEL_SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dir, classes):\n",
    "        self.dir = join(\"scripts\", dir)\n",
    "        self.classes = classes\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    Dataset(\n",
    "        dir=\"ten_newsgroups_graphs\",\n",
    "        classes=['business', 'entertainment', 'food', 'graphics', 'historical', 'medical', 'politics', 'space', 'sport', 'technologie']\n",
    "    ),\n",
    "\n",
    "    Dataset(\n",
    "        dir=\"bbcsport_graphs\",\n",
    "        classes=['athletics', 'cricket', 'football', 'rugby', 'tennis']\n",
    "    )\n",
    "]\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    makedirs(dataset.dir, exist_ok=True)\n",
    "\n",
    "#SAMPLE_SIZES = [1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000, 25000, 50000]\n",
    "SAMPLE_SIZES = [10, 50, 100, 500, 1000, 5000]\n",
    "\n",
    "NODE_TYPES = ['article', 'author', 'organization', 'topic']\n",
    "EDGE_TYPES = ['cites_by_test1', 'mentions_to_test2', 'authored_by_test3', 'belongs_to_test4', 'related_to_test5']\n",
    "\n",
    "NOTE_DISTRIBUTION = {\n",
    "    'article': 0.6,\n",
    "    'author': 0.2,\n",
    "    'organization': 0.1,\n",
    "    'topic': 0.1\n",
    "}\n",
    "\n",
    "EDGE_DISTRIBUTION = [\n",
    "    0.3,\n",
    "    0.25,\n",
    "    0.2,\n",
    "    0.15,\n",
    "    0.1\n",
    "]\n",
    "\n",
    "TRAIN_TEST_RATIO = 0.7\n",
    "MIN_NODES = 5\n",
    "MAX_NODES = 30\n",
    "\n",
    "NUM_WORKERS = cpu_count() // 2\n",
    "LOCK = Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(*args):\n",
    "    \"\"\"Универсальное объединение путей через pathlib\"\"\"\n",
    "    return Path(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path, exist_ok=False):\n",
    "    \"\"\"Создает директорию с родительскими, если необходимо\"\"\"\n",
    "    path = Path(path)\n",
    "    path.mkdir(parents=True, exist_ok=exist_ok)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_paths(datasets):\n",
    "    \"\"\"Создает всю структуру директорий заранее\"\"\"\n",
    "    required_paths = [\n",
    "        join(BASE_DIR, dataset.dir, class_name)\n",
    "        for dataset in datasets\n",
    "        for class_name in dataset.classes\n",
    "    ]\n",
    "    \n",
    "    for path in required_paths:\n",
    "        makedirs(path, exist_ok=True)\n",
    "        if not path.exists():\n",
    "            raise RuntimeError(f\"Failed to create directory: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_save_labels(class_dir, class_name, node_types, edge_types):\n",
    "    \"\"\"Потокобезопасное сохранение меток\"\"\"\n",
    "    class_path = Path(class_dir)\n",
    "    \n",
    "    # Исправлено создание словаря для vertex_labels\n",
    "    vertex_labels = {node_type: idx for idx, node_type in enumerate(node_types)}\n",
    "    vertex_path = join(class_path, f\"{class_name}_vertex_labels.pickle\")\n",
    "    with vertex_path.open('wb') as f:\n",
    "        pickle.dump(vertex_labels, f)\n",
    "    \n",
    "    # Исправлено создание словаря для edge_labels\n",
    "    edge_labels = {tuple(et.split('_')): idx for idx, et in enumerate(edge_types)}\n",
    "    edge_path = join(class_path, f\"{class_name}_edge_labels.pickle\")\n",
    "    with edge_path.open('wb') as f:\n",
    "        pickle.dump(edge_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_patterns():\n",
    "    \"\"\"Генерирует и сохраняет концепты и частые подграфы с проверкой наличия vertex_labels\"\"\"\n",
    "    for dataset in DATASETS:\n",
    "        for class_name in dataset.classes:\n",
    "            class_dir = Path(BASE_DIR) / dataset.dir / class_name\n",
    "            class_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            safe_save_labels(class_dir, class_name, NODE_TYPES, EDGE_TYPES)\n",
    "\n",
    "            # Проверка наличия vertex_labels\n",
    "            vertex_path = join(class_dir, f\"{class_name}_vertex_labels.pickle\")\n",
    "            if not vertex_path.exists():\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Vertex labels missing for {class_name}! \"\n",
    "                    f\"Expected at: {vertex_path}\"\n",
    "                )\n",
    "\n",
    "            # Загрузка меток\n",
    "            with vertex_path.open('rb') as f:\n",
    "                vertex_labels = pickle.load(f)\n",
    "\n",
    "            edge_path = join(class_dir, f\"{class_name}_edge_labels.pickle\")\n",
    "            with open(edge_path, 'rb') as f:\n",
    "                edge_labels = pickle.load(f)\n",
    "\n",
    "            # Генерация 40 концептов для каждого класса\n",
    "            concepts = []\n",
    "            for i in range(40):\n",
    "                concept = {\n",
    "                    'subgraphs': [\n",
    "                        f\"{random.choice(['node', 'edge'])}{j}-{random.randint(0, 100)}\"\n",
    "                        for j in range(random.randint(2, 5))\n",
    "                    ],\n",
    "                    'supports': [\n",
    "                        round(random.uniform(0.1, 1.0), 2)\n",
    "                        for _ in range(random.randint(2, 4))\n",
    "                    ],\n",
    "                    'extent': [\n",
    "                        str(random.randint(100, 1000))\n",
    "                        for _ in range(random.randint(3, 6))\n",
    "                    ],\n",
    "                    'id': f\"{class_name}_concept_{i}\"\n",
    "                }\n",
    "                concepts.append(concept)\n",
    "\n",
    "            # Генерация частых подграфов\n",
    "            # Генерация частых подграфов (с защитой от пустых структур)\n",
    "            frequent_subgraphs = []\n",
    "            for i in range(30):\n",
    "                G = nx.MultiDiGraph()\n",
    "                \n",
    "                # Гарантируем минимум 2 узла и 1 ребро\n",
    "                num_nodes = random.randint(2, 6)\n",
    "                num_edges = random.randint(1, num_nodes * (num_nodes - 1))\n",
    "                \n",
    "                # Добавляем узлы с гарантированными метками\n",
    "                valid_labels = list(vertex_labels.keys())\n",
    "                if not valid_labels:\n",
    "                    raise ValueError(f\"No vertex labels for {class_name}\")\n",
    "                \n",
    "                for node_id in range(num_nodes):\n",
    "                    G.add_node(node_id, label=random.choice(valid_labels))\n",
    "                \n",
    "                # Добавляем ребра с гарантированными парами узлов\n",
    "                all_pairs = list(permutations(range(num_nodes), 2))\n",
    "                edges_to_add = random.sample(all_pairs, min(num_edges, len(all_pairs)))\n",
    "                \n",
    "                for u, v in edges_to_add:\n",
    "                    edge_type = random.choice(list(edge_labels.keys()))\n",
    "                    G.add_edge(u, v, label=edge_type)\n",
    "                \n",
    "                # Пропускаем графы без ребер (защита)\n",
    "                if len(G.edges()) == 0:\n",
    "                    continue\n",
    "                \n",
    "                frequent_subgraphs.append({\n",
    "                    'id': f\"{class_name}_frequent_subgraph_{i}\",\n",
    "                    'subgraphs': [G],\n",
    "                    'extent': [str(random.randint(1, 100)) for _ in range(5, 12)]\n",
    "                })\n",
    "\n",
    "            # Генерация паттернов\n",
    "            patterns = []\n",
    "            vertex_labels_list = list(vertex_labels.values())\n",
    "            edge_labels_list = list(edge_labels.values())\n",
    "\n",
    "            for pattern_id in range(40):\n",
    "                num_nodes = random.randint(1, 5)\n",
    "                nodes = [\n",
    "                    (node_id, random.choice(vertex_labels_list))\n",
    "                    for node_id in range(num_nodes)\n",
    "                ]\n",
    "\n",
    "                edges = []\n",
    "                if num_nodes > 1:\n",
    "                    num_edges = random.randint(0, min(4, num_nodes * 2))\n",
    "                    for _ in range(num_edges):\n",
    "                        u, v = random.sample(range(num_nodes), 2)\n",
    "                        edge_type = random.choice(edge_labels_list)\n",
    "                        edges.append((u, v, edge_type))\n",
    "\n",
    "                graph_ids = random.sample(range(100), random.randint(10, 20))\n",
    "\n",
    "                pattern = [\n",
    "                    f\"# {random.randint(10, 100)}\",\n",
    "                    f\"t # {pattern_id}\",\n",
    "                    *[f\"v {node_id} {label}\" for node_id, label in nodes],\n",
    "                    *[f\"e {u} {v} {label}\" for u, v, label in edges],\n",
    "                    f\"x {' '.join(map(str, graph_ids))}\"\n",
    "                ]\n",
    "                patterns.extend(pattern)\n",
    "\n",
    "            # Сохранение файлов внутри цикла по классам\n",
    "            concepts_path = join(class_dir, f'{class_name}_concepts.pickle')\n",
    "            with open(concepts_path, 'wb') as f:\n",
    "                pickle.dump(concepts, f)\n",
    "\n",
    "            frequent_subgraphs_path = join(class_dir, f'{class_name}_frequent_subgraphs.pickle')\n",
    "            with open(frequent_subgraphs_path, 'wb') as f:\n",
    "                pickle.dump(frequent_subgraphs, f)\n",
    "\n",
    "            # Исправлено: сохранение в текстовом формате вместо pickle\n",
    "            patterns_out_path = join(class_dir, f'{class_name}_patterns.OUT')\n",
    "            with open(patterns_out_path, 'w') as f:\n",
    "                f.write('\\n'.join(patterns))  # Записываем строки как текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_lattice_file(class_dir, class_name):\n",
    "    \"\"\"Генерирует и сохраняет файл lattice для указанного класса.\"\"\"\n",
    "    lattice_structure = [\n",
    "        {\n",
    "            \"NodesCount\": random.randint(50, 150),\n",
    "            \"ArcsCount\": random.randint(100, 300),\n",
    "            \"Top\": [random.randint(0, 50)],\n",
    "            \"Bottom\": random.sample(range(0, 150), 25),\n",
    "            \"Params\": {\n",
    "                \"Type\": \"ContextProcessorModules\",\n",
    "                \"Name\": \"SofiaContextProcessorModule\",\n",
    "                \"Params\": {\n",
    "                    \"Thld\": 5.0,\n",
    "                    \"MaxPatternNumber\": 10000,\n",
    "                    \"AdjustThreshold\": True,\n",
    "                    \"FindPartialOrder\": True,\n",
    "                    \"OutputParams\": {\"OutExtent\": True, \"OutIntent\": True},\n",
    "                    \"ProjectionChain\": {\n",
    "                        \"Type\": \"ProjectionChainModules\",\n",
    "                        \"Name\": \"StabClsPatternProjectionChainModule\",\n",
    "                        \"Params\": {\n",
    "                            \"ImageReserve\": 31000,\n",
    "                            \"Enumerator\": {\n",
    "                                \"Type\": \"PatternEnumeratorModules\",\n",
    "                                \"Name\": \"ParallelPatternEnumeratorModule\",\n",
    "                                \"Params\": {\n",
    "                                    \"PatternEnumeratorByCallback\": {\n",
    "                                        \"Type\": \"PatternEnumeratorModules\",\n",
    "                                        \"Name\": \"GastonGraphPatternEnumeratorModule\",\n",
    "                                        \"Params\": {\n",
    "                                            \"LibPath\": \"libGaston.so\",\n",
    "                                            \"InputPath\": f\"../../../../LibGastonForSofia/sample-data/{class_name}_train_gsofia\",\n",
    "                                            \"RemoveInput\": False,\n",
    "                                            \"PatternPath\": f\"{class_name}_patterns.OUT\",\n",
    "                                            \"MinSupport\": 18,\n",
    "                                            \"MaxGraphSize\": -1,\n",
    "                                            \"GastonMode\": \"Full\"\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Nodes\": [\n",
    "                {\n",
    "                    \"Ext\": {\n",
    "                        \"Count\": random.randint(20, 100),\n",
    "                        \"Inds\": random.sample(range(100), random.randint(15, 30))\n",
    "                    },\n",
    "                    \"Int\": {\n",
    "                        \"GraphCount\": random.randint(10, 50),\n",
    "                        \"MinGraphSupport\": random.randint(20, 100)\n",
    "                    },\n",
    "                    \"Interest\": random.randint(5, 15)\n",
    "                } for _ in range(40)\n",
    "            ],\n",
    "            \"Arcs\": [\n",
    "                {\n",
    "                    \"S\": random.randint(0, 100),\n",
    "                    \"D\": random.randint(0, 100)\n",
    "                } for _ in range(200)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    lattice_path = Path(class_dir) / f\"{class_name}_lattice.json\"\n",
    "    with lattice_path.open('w', encoding='utf-8') as f:\n",
    "        json.dump(lattice_structure, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_file_operations(path, content):\n",
    "    \"\"\"Безопасная запись файлов с проверкой контрольных сумм\"\"\"\n",
    "    temp_path = f\"{path}.tmp\"\n",
    "    with open(temp_path, 'wb') as f:\n",
    "        pickle.dump(content, f)\n",
    "    \n",
    "    with open(temp_path, 'rb') as f:\n",
    "        checksum = hashlib.md5(f.read()).hexdigest()\n",
    "    \n",
    "    Path(temp_path).replace(path)\n",
    "    return checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dynamic_probs(n):\n",
    "    \"\"\"Генерирует нормализованные убывающие вероятности для заданного количества элементов\"\"\"\n",
    "    if n <= 0:\n",
    "        return np.array([])\n",
    "    base = np.exp(-0.5 * np.arange(n))\n",
    "    base /= base.sum()\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_worker():\n",
    "    \"\"\"Инициализация генератора случайных чисел для каждого процесса\"\"\"\n",
    "    np.random.seed((getpid() * int(time.time())) % 123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(args, num_graphs):\n",
    "    init_paths(DATASETS)\n",
    "    \n",
    "    class_dir, class_name, node_types, node_probs, edge_probs, i = args\n",
    "    G = nx.DiGraph()\n",
    "    prefix = 'train' if i < num_graphs * TRAIN_TEST_RATIO else 'test'\n",
    "\n",
    "    num_nodes = random.randint(MIN_NODES, MAX_NODES)\n",
    "    if num_nodes == 0:\n",
    "        num_nodes = 1  # Гарантируем хотя бы одну вершину\n",
    "\n",
    "    for node_id in range(num_nodes):\n",
    "        node_type = np.random.choice(node_types, p=node_probs)\n",
    "        content = f\"{node_type.capitalize()} about {class_name} - {np.random.randint(1000, 9999)}\"\n",
    "        G.add_node(\n",
    "            node_id,\n",
    "            label=node_type,\n",
    "            content=content,\n",
    "            class_label=class_name,\n",
    "            creation_date=f\"2023-{random.randint(1,12):02d}-{random.randint(1,28):02d}\"\n",
    "        )\n",
    "\n",
    "    all_nodes = list(G.nodes())\n",
    "    if len(all_nodes) > 1:\n",
    "        for u in all_nodes:\n",
    "            if random.random() < 0.7:\n",
    "                targets = [n for n in all_nodes if G.nodes[n]['class_label'] == class_name]\n",
    "            else:\n",
    "                targets = all_nodes\n",
    "            \n",
    "            if targets and u in targets:\n",
    "                targets.remove(u)  # Исключаем саму вершину\n",
    "                n_targets = len(targets)\n",
    "                \n",
    "                if n_targets > 0:\n",
    "                    probs = generate_dynamic_probs(n_targets)\n",
    "                    try:\n",
    "                        selected = np.random.choice(\n",
    "                            targets,\n",
    "                            size=min(3, n_targets),\n",
    "                            replace=False,\n",
    "                            p=probs\n",
    "                        )\n",
    "                        for v in selected:\n",
    "                            edge_type = np.random.choice(EDGE_TYPES, p=edge_probs)\n",
    "                            weight = round(np.random.beta(2, 5), 2)\n",
    "                            G.add_edge(\n",
    "                                u, v,\n",
    "                                label=edge_type,\n",
    "                                weight=weight,\n",
    "                                timestamp=np.random.randint(1609459200, 1672531200)\n",
    "                            )\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "    for node in G.nodes(data=True):\n",
    "        node[1]['content'] = str(node[1]['content'])\n",
    "        node[1]['creation_date'] = str(node[1]['creation_date'])\n",
    "\n",
    "    graph_path = join(class_dir, f'{class_name}_{prefix}_graph_{i}.gml')\n",
    "    with LOCK:\n",
    "        nx.write_gml(G, graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_data_dirs():\n",
    "    for dataset in DATASETS:\n",
    "        for class_name in dataset.classes:\n",
    "            class_dir = join(BASE_DIR, dataset.dir, class_name)\n",
    "            if class_dir.exists():\n",
    "                rmtree(class_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_generate_gSOFIA_data(num_graphs):\n",
    "    \"\"\"Генерация графов и структуры данных с настраиваемым количеством графов\"\"\"\n",
    "    assert abs(sum(NOTE_DISTRIBUTION.values()) - 1.0) < 1e-6, \"Node distribution error\"\n",
    "    assert abs(sum(EDGE_DISTRIBUTION) - 1.0) < 1e-6, \"Edge distribution error\"\n",
    "\n",
    "    node_types = list(NOTE_DISTRIBUTION.keys())\n",
    "    node_probs = list(NOTE_DISTRIBUTION.values())\n",
    "    edge_probs = list(EDGE_DISTRIBUTION)\n",
    "\n",
    "    total_classes = sum([len(dataset.classes) for dataset in DATASETS])\n",
    "    print(\"Total classes:\", total_classes)\n",
    "    total_graphs_per_class =  int(round(num_graphs * (1 + TRAIN_TEST_RATIO), 0)) // total_classes\n",
    "\n",
    "    tasks = []\n",
    "    for dataset in DATASETS:\n",
    "        futures = []\n",
    "        for class_name in dataset.classes:\n",
    "            class_dir = join(BASE_DIR, dataset.dir, class_name)\n",
    "            makedirs(class_dir, exist_ok=True)\n",
    "            \n",
    "            safe_save_labels(class_dir, class_name, node_types, EDGE_TYPES)\n",
    "            \n",
    "            # Генерация графов (train + test)\n",
    "            \n",
    "            \n",
    "            for i in range(total_graphs_per_class):\n",
    "                # prefix = 'train' if i < num_graphs * TRAIN_TEST_RATIO else 'test'\n",
    "                # graph_path = join(class_dir, f'{class_name}_{prefix}_graph_{i}.gml')\n",
    "                \n",
    "                # Генерация и сохранение графа\n",
    "                futures.append(\n",
    "                    generate_graph((str(class_dir), class_name, node_types, node_probs, edge_probs, i), total_graphs_per_class)\n",
    "                )\n",
    "\n",
    "            generate_lattice_file(str(class_dir), class_name)\n",
    "\n",
    "            total_classes += 1\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=NUM_WORKERS, initializer=init_worker) as executor:\n",
    "            # futures = [executor.submit(generate_graph, task, num_graphs=num_graphs // len(dataset.classes)) for task in tasks]\n",
    "            for _ in tqdm(futures, total=len(futures), desc=\"Generating graphs\"):\n",
    "                pass\n",
    "\n",
    "    generate_and_save_patterns()\n",
    "    print(f\"Generated data for {total_graphs_per_class * total_classes} graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to run scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_script(script_func):\n",
    "    \"\"\"Запуск с правильными параметрами для вашей системы\"\"\"\n",
    "\n",
    "    root = os.getcwd()\n",
    "    start_time = time.time()\n",
    "\n",
    "    config = join(root, 'config', 'config.yaml')\n",
    "    with open(config) as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    dataset = \"all\"\n",
    "    mode = \"all\"\n",
    "    weighting = \"no\"\n",
    "    graph_pattern_reconstruction = \"no\"\n",
    "\n",
    "    try:\n",
    "        script_func(root=root,\n",
    "                    config=config,\n",
    "                    dataset=dataset,\n",
    "                    mode=mode,\n",
    "                    weighting=weighting,\n",
    "                    graph_pattern_reconstruction=graph_pattern_reconstruction)\n",
    "            \n",
    "    except CalledProcessError as e:\n",
    "        print(f\"Process error in {script_func}:\\n{e.stderr}\")\n",
    "        return float('inf')\n",
    "\n",
    "    except TimeoutExpired:\n",
    "        print(f\"Timeout in {script_func}\")\n",
    "        return float('inf')\n",
    "    \n",
    "    return time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация тестовых данных\n",
    "# print(\"Generating test data...\")\n",
    "# for size in tqdm(SAMPLE_SIZES):\n",
    "#     parallel_generate_gSOFIA_data(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116899a981674b43b94ecfa23ab1d683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating graphs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa2e9594d144e21b08f537efc5d7aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating graphs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data for 30 graphs\n"
     ]
    }
   ],
   "source": [
    "delete_data_dirs()\n",
    "parallel_generate_gSOFIA_data(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1555df69d44e4c8fb2fffd6d1b8e27c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9383dfa1b0435b8f3b2ef9cd862338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating graphs:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring for ten_newsgroups for concepts done.\n",
      "{'dataset': 'ten_newsgroups', 'mode': 'concepts', 'baseline_penalty_macro-averaged_f1-score': 0.6424603339706314, 'penalty_1_macro-averaged_f1-score': 0.6932400698716489, 'penalty_2_macro-averaged_f1-score': 0.6765980400897618, 'penalty_3_macro-averaged_f1-score': 0.6453613579403052, 'edge_penalty_macro-averaged_f1-score': 0.5259470797401831, 'penalty_5_macro-averaged_f1-score': 0.667410319470691, 'penalty_6_macro-averaged_f1-score': 0.7114699627857523}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring for ten_newsgroups for equivalence classes done.\n",
      "{'dataset': 'ten_newsgroups', 'mode': 'equivalence_classes', 'baseline_penalty_macro-averaged_f1-score': 0.6714201114104836, 'penalty_1_macro-averaged_f1-score': 0.7957093714678853, 'penalty_2_macro-averaged_f1-score': 0.7735057340320497, 'penalty_3_macro-averaged_f1-score': 0.7749985686827792, 'edge_penalty_macro-averaged_f1-score': 0.32437557699752817, 'penalty_5_macro-averaged_f1-score': 0.7876013543896165, 'penalty_6_macro-averaged_f1-score': 0.7739166097060833}\n",
      "Scoring for ten_newsgroups for frequent subgraphs done.\n",
      "{'dataset': 'ten_newsgroups', 'mode': 'frequent_subgraphs', 'baseline_penalty_macro-averaged_f1-score': 0.7411716155100191, 'penalty_1_macro-averaged_f1-score': 0.8246124262061805, 'penalty_2_macro-averaged_f1-score': 0.8016934381408065, 'penalty_3_macro-averaged_f1-score': 0.7766740360938773, 'edge_penalty_macro-averaged_f1-score': 0.6266143893890324, 'penalty_5_macro-averaged_f1-score': 0.82870074560578, 'penalty_6_macro-averaged_f1-score': 0.8020698732998504}\n",
      "Scoring for bbcsport for concepts done.\n",
      "{'dataset': 'bbcsport', 'mode': 'concepts', 'baseline_penalty_macro-averaged_f1-score': 0.7884120710207666, 'penalty_1_macro-averaged_f1-score': 0.9502683141089298, 'penalty_2_macro-averaged_f1-score': 0.8855197132616489, 'penalty_3_macro-averaged_f1-score': 0.8857692307692308, 'edge_penalty_macro-averaged_f1-score': 0.6829457669355368, 'penalty_5_macro-averaged_f1-score': 0.8732248594317559, 'penalty_6_macro-averaged_f1-score': 0.9502683141089298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring for bbcsport for equivalence classes done.\n",
      "{'dataset': 'bbcsport', 'mode': 'equivalence_classes', 'baseline_penalty_macro-averaged_f1-score': 0.7413663243075008, 'penalty_1_macro-averaged_f1-score': 0.9227173113624726, 'penalty_2_macro-averaged_f1-score': 0.9045502645502645, 'penalty_3_macro-averaged_f1-score': 0.8941479484875711, 'edge_penalty_macro-averaged_f1-score': 0.5437306949241847, 'penalty_5_macro-averaged_f1-score': 0.9084054834054834, 'penalty_6_macro-averaged_f1-score': 0.9299900386351998}\n",
      "Scoring for bbcsport for frequent subgraphs done.\n",
      "{'dataset': 'bbcsport', 'mode': 'frequent_subgraphs', 'baseline_penalty_macro-averaged_f1-score': 0.7012962962962963, 'penalty_1_macro-averaged_f1-score': 0.9299900386351998, 'penalty_2_macro-averaged_f1-score': 0.9299900386351998, 'penalty_3_macro-averaged_f1-score': 0.9186996993448606, 'edge_penalty_macro-averaged_f1-score': 0.9436090225563909, 'penalty_5_macro-averaged_f1-score': 0.8639122315592903, 'penalty_6_macro-averaged_f1-score': 0.9299900386351998}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(score_graph_patterns pid=4328)\u001b[0m Scoring graph patterns for ten_newsgroups...\n",
      "\u001b[36m(score_graph_patterns pid=13536)\u001b[0m Scoring graph patterns for bbcsport...\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 40 PYTHON worker processes have been started on node: 9776bd256faade10808b6b64054a21f34947db306d27fad668b32bf8 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 50 PYTHON worker processes have been started on node: 9776bd256faade10808b6b64054a21f34947db306d27fad668b32bf8 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 60 PYTHON worker processes have been started on node: 9776bd256faade10808b6b64054a21f34947db306d27fad668b32bf8 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 70 PYTHON worker processes have been started on node: 9776bd256faade10808b6b64054a21f34947db306d27fad668b32bf8 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 88 PYTHON worker processes have been started on node: 9776bd256faade10808b6b64054a21f34947db306d27fad668b32bf8 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 101 PYTHON worker processes have been started on node: 9776bd256faade10808b6b64054a21f34947db306d27fad668b32bf8 with address: 127.0.0.1. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(process_graph_class pid=25792)\u001b[0m Class food training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=14992)\u001b[0m Class entertainment training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=30616)\u001b[0m Class graphics training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=28060)\u001b[0m Class medical training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=3120)\u001b[0m Class business training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=13512)\u001b[0m Class sport training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=26600)\u001b[0m Class politics training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=8616)\u001b[0m Class space training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=13800)\u001b[0m Class technologie training graphs finished.\n",
      "\u001b[36m(process_graph_class pid=31412)\u001b[0m Class historical training graphs finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-02-27 19:25:04,124 C 4908 13616] dlmalloc.cc:129:  Check failed: *handle != nullptr CreateFileMapping() failed. GetLastError() = 1450\n",
      "\u001b[33m(raylet)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[33m(raylet)\u001b[0m unknown\n",
      "\u001b[33m(raylet)\u001b[0m \n"
     ]
    },
    {
     "ename": "LocalRayletDiedError",
     "evalue": "The task's local raylet died. Check raylet.out for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalRayletDiedError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m\n\u001b[0;32m     15\u001b[0m original_times\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     16\u001b[0m     run_script(ORIGINAL_SCRIPT)\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Параллельная версия\u001b[39;00m\n\u001b[0;32m     20\u001b[0m parallel_times\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mrun_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARALLEL_SCRIPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m )\n",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m, in \u001b[0;36mrun_script\u001b[1;34m(script_func)\u001b[0m\n\u001b[0;32m     14\u001b[0m graph_pattern_reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mscript_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweighting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgraph_pattern_reconstruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_pattern_reconstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess error in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\project\\scripts\\paralized\\main.py:59\u001b[0m, in \u001b[0;36mdo_operations\u001b[1;34m(root, config, dataset, mode, weighting, graph_pattern_reconstruction)\u001b[0m\n\u001b[0;32m     52\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     53\u001b[0m         weight_graph_patterns\u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mten_newsgroups\u001b[39m\u001b[38;5;124m'\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mten_newsgroups_classes\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mstr\u001b[39m(root) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mten_newsgroups_data_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m], mode)\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     55\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     56\u001b[0m         weight_graph_patterns\u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbcsport\u001b[39m\u001b[38;5;124m'\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbcsport_classes\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mstr\u001b[39m(root) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbcsport_data_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m], mode)\n\u001b[0;32m     57\u001b[0m     )\n\u001b[1;32m---> 59\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_graph_patterns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mten_newsgroups\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mten_newsgroups_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mten_newsgroups_data_prefix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_graph_patterns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbbcsport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbbcsport_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbbcsport_data_prefix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores)\n\u001b[0;32m     66\u001b[0m construct_results_dataframe(root, scores, config)\n",
      "File \u001b[1;32mc:\\project\\.venv\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     20\u001b[0m     auto_init_ray()\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\project\\.venv\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\project\\.venv\\Lib\\site-packages\\ray\\_private\\worker.py:2772\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2767\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2768\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2769\u001b[0m     )\n\u001b[0;32m   2771\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[1;32m-> 2772\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[0;32m   2774\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[1;32mc:\\project\\.venv\\Lib\\site-packages\\ray\\_private\\worker.py:921\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[1;34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[0;32m    920\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 921\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values, debugger_breakpoint\n",
      "\u001b[1;31mLocalRayletDiedError\u001b[0m: The task's local raylet died. Check raylet.out for more information."
     ]
    }
   ],
   "source": [
    "# Замер производительности\n",
    "original_times = []\n",
    "parallel_times = []\n",
    "\n",
    "print(\"Benchmarking...\")\n",
    "for size in tqdm(SAMPLE_SIZES):\n",
    "\n",
    "    # Удаляем старые данные\n",
    "    delete_data_dirs()\n",
    "\n",
    "    # Генерируем новые данные\n",
    "    parallel_generate_gSOFIA_data(size)\n",
    "    \n",
    "    # Оригинальная версия\n",
    "    original_times.append(\n",
    "        run_script(ORIGINAL_SCRIPT)\n",
    "    )\n",
    "    \n",
    "    # Параллельная версия\n",
    "    parallel_times.append(\n",
    "        run_script(PARALLEL_SCRIPT)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация результатов\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(SAMPLE_SIZES, original_times, marker='o', label='Original')\n",
    "plt.plot(SAMPLE_SIZES, parallel_times, marker='x', label='Parallel')\n",
    "plt.xlabel('Number of Graphs')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет ускорения\n",
    "speedup = np.array(original_times) / np.array(parallel_times)\n",
    "print(f\"Average speedup: {np.mean(speedup):.2f}x\")\n",
    "print(f\"Max speedup: {np.max(speedup):.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
